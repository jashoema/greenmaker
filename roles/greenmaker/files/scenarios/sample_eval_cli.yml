# Usage: ansible-playbook test_greenmaker.yml -l <target_host> -i <inventory_file> -e '{ "target_scenarios":["sample_eval_cli"] }'
---
# "metadata" provides descriptive information about the scenario, such as its purpose and the actions that it takes
metadata:
  id: "100001"
  version: "1"
  name: "sample_eval_cli"
  description: |
    This scenario demonstrates usage of the eval_cli action by executing a "show" command against the target device,
    looking for the presence of a particular regular expression in the "show" command output, extracting data from the output,
    assigning the extracted data to a variable, and finally echoing the variable to present the extracted data.
  product_family:
    - "All"
  severity: "3"
  device_roles:
    - "TBD"
  categories: []
  related_defects: []
  date_modified: ""
  alert_created: false
  troubleshooting_actions: ""
 
# "triggers" describes the network event(s) or trigger conditions upon which the scenario workflow will be executed.
# This sample scenario will be manually triggered, so we leave the triggers list empty.
triggers: []

# "workflow" provides a list of steps that will be executed by the auto-remediation workflow engine (e.g. Greenmaker role in Ansible)
workflow:
  - step:
      metadata:
        name: "eval_cli_capture"
        version: "1.0.0"
        description: |
          Executing a "show failure" command against the target device and look for the presence of a failure message. If failure message
          is present, extract the failure code, and echo the failure code to the logs.
      validation:
        eval_cli:
          args:
            commands:
              - show failure
            pattern: "Reason: ([\\s\\S]+) ([\\s\\S]+) ([\\s\\S]+)"
          output:
            - source: pattern_search[0]
              target: failure_code
            - source: pattern_search[1]
              target: failure_code1
            - source: pattern_search[2]
              target: failure_code2
          settings:
            test:
              input: eval_cli_capture_test_input
      on_true:
        - echo: 
            args:
              message: |
                {{ failure_code }} {{ failure_code1 }} {{ failure_code2 }}
        - exit:
            args:
              reason: "Execution complete. eval_cli found the desired pattern and extracted failure code {{ failure_code }}"
      on_false:
        - echo:
            args:
              message: |
                eval_cli did not find the desired pattern in the "show" command output
        - exit:
            args:
              reason: Unable to find desired pattern in the "show" command output.

# "tests" describes available tests that can be executed for this scenario in Test Mode and any relevant simulated data for the test  
tests: 
  validation:
    - metadata:
        name: default_test
        description: | 
          Test execution of eval_cli action. Inject cli_eval_data as simulated output for "show failure" command.
      extra_vars:
        - alert_vars: {}
      input:
        eval_cli_capture_test_input:
          cli_eval_data: |
            !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
            Reason: Module restarted yes
            !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
          expected_result: 
            return: true

library: {}
